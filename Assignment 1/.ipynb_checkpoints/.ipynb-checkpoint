{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Comparison of Multivariate Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n",
    "*In this assignment, you will get to pick your favorite data set on Kaggle (https://kaggle.com) and implement some learning models in the context of a **regression** problem.  For references, you may refer to [my slides](https://docs.google.com/presentation/d/10D1he89peAWaFgjtZlHpUzvOOAie_vIFT95htKCKgc0/edit#slide=id.p) or Chapter 4 of the textbook if you need additional sample codes to help with your assignment. To get started, you will need to determine which dataset to download and copy it into the directory where you wish to run your implementation (ie. same folder as this file).* \n",
    "\n",
    "*For deliverables, you must write code in Python and submit **this** Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. BIG PICTURE (5 pts)\n",
    "Write a paragraph explaining the context of the problem in which you are trying to investigate. In the same paragraph, explain why you pick your dataset. Then, you MUST include the URL to the dataset to help the TA to download it (we will assume that the data file is put into the same folder as the Jupyter Notebook file). Then, you can write some code to load the CSV file and take a quick look at the dataset, and output the following:\n",
    "\n",
    " * How big is your dataset? (in terms of MB)\n",
    " * How many entries does it have?\n",
    " * How many features does it have?\n",
    " * What are some basic statistics you can learn right away about this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0               7.4             0.700         0.00             1.9      0.076   \n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "5               7.4             0.660         0.00             1.8      0.075   \n",
      "6               7.9             0.600         0.06             1.6      0.069   \n",
      "7               7.3             0.650         0.00             1.2      0.065   \n",
      "8               7.8             0.580         0.02             2.0      0.073   \n",
      "9               7.5             0.500         0.36             6.1      0.071   \n",
      "10              6.7             0.580         0.08             1.8      0.097   \n",
      "11              7.5             0.500         0.36             6.1      0.071   \n",
      "12              5.6             0.615         0.00             1.6      0.089   \n",
      "13              7.8             0.610         0.29             1.6      0.114   \n",
      "14              8.9             0.620         0.18             3.8      0.176   \n",
      "15              8.9             0.620         0.19             3.9      0.170   \n",
      "16              8.5             0.280         0.56             1.8      0.092   \n",
      "17              8.1             0.560         0.28             1.7      0.368   \n",
      "18              7.4             0.590         0.08             4.4      0.086   \n",
      "19              7.9             0.320         0.51             1.8      0.341   \n",
      "20              8.9             0.220         0.48             1.8      0.077   \n",
      "21              7.6             0.390         0.31             2.3      0.082   \n",
      "22              7.9             0.430         0.21             1.6      0.106   \n",
      "23              8.5             0.490         0.11             2.3      0.084   \n",
      "24              6.9             0.400         0.14             2.4      0.085   \n",
      "25              6.3             0.390         0.16             1.4      0.080   \n",
      "26              7.6             0.410         0.24             1.8      0.080   \n",
      "27              7.9             0.430         0.21             1.6      0.106   \n",
      "28              7.1             0.710         0.00             1.9      0.080   \n",
      "29              7.8             0.645         0.00             2.0      0.082   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1569            6.2             0.510         0.14             1.9      0.056   \n",
      "1570            6.4             0.360         0.53             2.2      0.230   \n",
      "1571            6.4             0.380         0.14             2.2      0.038   \n",
      "1572            7.3             0.690         0.32             2.2      0.069   \n",
      "1573            6.0             0.580         0.20             2.4      0.075   \n",
      "1574            5.6             0.310         0.78            13.9      0.074   \n",
      "1575            7.5             0.520         0.40             2.2      0.060   \n",
      "1576            8.0             0.300         0.63             1.6      0.081   \n",
      "1577            6.2             0.700         0.15             5.1      0.076   \n",
      "1578            6.8             0.670         0.15             1.8      0.118   \n",
      "1579            6.2             0.560         0.09             1.7      0.053   \n",
      "1580            7.4             0.350         0.33             2.4      0.068   \n",
      "1581            6.2             0.560         0.09             1.7      0.053   \n",
      "1582            6.1             0.715         0.10             2.6      0.053   \n",
      "1583            6.2             0.460         0.29             2.1      0.074   \n",
      "1584            6.7             0.320         0.44             2.4      0.061   \n",
      "1585            7.2             0.390         0.44             2.6      0.066   \n",
      "1586            7.5             0.310         0.41             2.4      0.065   \n",
      "1587            5.8             0.610         0.11             1.8      0.066   \n",
      "1588            7.2             0.660         0.33             2.5      0.068   \n",
      "1589            6.6             0.725         0.20             7.8      0.073   \n",
      "1590            6.3             0.550         0.15             1.8      0.077   \n",
      "1591            5.4             0.740         0.09             1.7      0.089   \n",
      "1592            6.3             0.510         0.13             2.3      0.076   \n",
      "1593            6.8             0.620         0.08             1.9      0.068   \n",
      "1594            6.2             0.600         0.08             2.0      0.090   \n",
      "1595            5.9             0.550         0.10             2.2      0.062   \n",
      "1596            6.3             0.510         0.13             2.3      0.076   \n",
      "1597            5.9             0.645         0.12             2.0      0.075   \n",
      "1598            6.0             0.310         0.47             3.6      0.067   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
      "6                    15.0                  59.0  0.99640  3.30       0.46   \n",
      "7                    15.0                  21.0  0.99460  3.39       0.47   \n",
      "8                     9.0                  18.0  0.99680  3.36       0.57   \n",
      "9                    17.0                 102.0  0.99780  3.35       0.80   \n",
      "10                   15.0                  65.0  0.99590  3.28       0.54   \n",
      "11                   17.0                 102.0  0.99780  3.35       0.80   \n",
      "12                   16.0                  59.0  0.99430  3.58       0.52   \n",
      "13                    9.0                  29.0  0.99740  3.26       1.56   \n",
      "14                   52.0                 145.0  0.99860  3.16       0.88   \n",
      "15                   51.0                 148.0  0.99860  3.17       0.93   \n",
      "16                   35.0                 103.0  0.99690  3.30       0.75   \n",
      "17                   16.0                  56.0  0.99680  3.11       1.28   \n",
      "18                    6.0                  29.0  0.99740  3.38       0.50   \n",
      "19                   17.0                  56.0  0.99690  3.04       1.08   \n",
      "20                   29.0                  60.0  0.99680  3.39       0.53   \n",
      "21                   23.0                  71.0  0.99820  3.52       0.65   \n",
      "22                   10.0                  37.0  0.99660  3.17       0.91   \n",
      "23                    9.0                  67.0  0.99680  3.17       0.53   \n",
      "24                   21.0                  40.0  0.99680  3.43       0.63   \n",
      "25                   11.0                  23.0  0.99550  3.34       0.56   \n",
      "26                    4.0                  11.0  0.99620  3.28       0.59   \n",
      "27                   10.0                  37.0  0.99660  3.17       0.91   \n",
      "28                   14.0                  35.0  0.99720  3.47       0.55   \n",
      "29                    8.0                  16.0  0.99640  3.38       0.59   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1569                 15.0                  34.0  0.99396  3.48       0.57   \n",
      "1570                 19.0                  35.0  0.99340  3.37       0.93   \n",
      "1571                 15.0                  25.0  0.99514  3.44       0.65   \n",
      "1572                 35.0                 104.0  0.99632  3.33       0.51   \n",
      "1573                 15.0                  50.0  0.99467  3.58       0.67   \n",
      "1574                 23.0                  92.0  0.99677  3.39       0.48   \n",
      "1575                 12.0                  20.0  0.99474  3.26       0.64   \n",
      "1576                 16.0                  29.0  0.99588  3.30       0.78   \n",
      "1577                 13.0                  27.0  0.99622  3.54       0.60   \n",
      "1578                 13.0                  20.0  0.99540  3.42       0.67   \n",
      "1579                 24.0                  32.0  0.99402  3.54       0.60   \n",
      "1580                  9.0                  26.0  0.99470  3.36       0.60   \n",
      "1581                 24.0                  32.0  0.99402  3.54       0.60   \n",
      "1582                 13.0                  27.0  0.99362  3.57       0.50   \n",
      "1583                 32.0                  98.0  0.99578  3.33       0.62   \n",
      "1584                 24.0                  34.0  0.99484  3.29       0.80   \n",
      "1585                 22.0                  48.0  0.99494  3.30       0.84   \n",
      "1586                 34.0                  60.0  0.99492  3.34       0.85   \n",
      "1587                 18.0                  28.0  0.99483  3.55       0.66   \n",
      "1588                 34.0                 102.0  0.99414  3.27       0.78   \n",
      "1589                 29.0                  79.0  0.99770  3.29       0.54   \n",
      "1590                 26.0                  35.0  0.99314  3.32       0.82   \n",
      "1591                 16.0                  26.0  0.99402  3.67       0.56   \n",
      "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
      "\n",
      "      alcohol  quality  \n",
      "0         9.4        5  \n",
      "1         9.8        5  \n",
      "2         9.8        5  \n",
      "3         9.8        6  \n",
      "4         9.4        5  \n",
      "5         9.4        5  \n",
      "6         9.4        5  \n",
      "7        10.0        7  \n",
      "8         9.5        7  \n",
      "9        10.5        5  \n",
      "10        9.2        5  \n",
      "11       10.5        5  \n",
      "12        9.9        5  \n",
      "13        9.1        5  \n",
      "14        9.2        5  \n",
      "15        9.2        5  \n",
      "16       10.5        7  \n",
      "17        9.3        5  \n",
      "18        9.0        4  \n",
      "19        9.2        6  \n",
      "20        9.4        6  \n",
      "21        9.7        5  \n",
      "22        9.5        5  \n",
      "23        9.4        5  \n",
      "24        9.7        6  \n",
      "25        9.3        5  \n",
      "26        9.5        5  \n",
      "27        9.5        5  \n",
      "28        9.4        5  \n",
      "29        9.8        6  \n",
      "...       ...      ...  \n",
      "1569     11.5        6  \n",
      "1570     12.4        6  \n",
      "1571     11.1        6  \n",
      "1572      9.5        5  \n",
      "1573     12.5        6  \n",
      "1574     10.5        6  \n",
      "1575     11.8        6  \n",
      "1576     10.8        6  \n",
      "1577     11.9        6  \n",
      "1578     11.3        6  \n",
      "1579     11.3        5  \n",
      "1580     11.9        6  \n",
      "1581     11.3        5  \n",
      "1582     11.9        5  \n",
      "1583      9.8        5  \n",
      "1584     11.6        7  \n",
      "1585     11.5        6  \n",
      "1586     11.4        6  \n",
      "1587     10.9        6  \n",
      "1588     12.8        6  \n",
      "1589      9.2        5  \n",
      "1590     11.6        6  \n",
      "1591     11.6        6  \n",
      "1592     11.0        6  \n",
      "1593      9.5        6  \n",
      "1594     10.5        5  \n",
      "1595     11.2        6  \n",
      "1596     11.0        6  \n",
      "1597     10.2        5  \n",
      "1598     11.0        6  \n",
      "\n",
      "[1599 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import some common packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Your code goes here for this section, make sure you also include the output to answer the above questions.\n",
    "datasetURL = \"https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\"; # required url to download for CSV file. The TA will need to download the file and run your program.\n",
    "data = pd.read_csv(\"winequality-red.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. DATA DISCOVERY, CLEANING, AND SCALING (10 pts)\n",
    "\n",
    "**Data Discover:** Plot out all correlations among the features. You may notice some features are more correlated with your predicted value than other. This information will help you confirm that weights of your regression model later on.\n",
    "\n",
    "**Data Cleaning:** If your dataset has some missing values, make sure you are able to fill those values with the Imputer class. If your dataset has categorical features, make sure you conver those features into numerical using OneHotEncoder class. \n",
    "\n",
    "**Feature Scaling** More importantly, your task is to write some codes to normalize the value of each features as follow:\n",
    "\n",
    "* Subtract the mean value of each feature from the dataset\n",
    "* Scale (divide) the feature values by their respective standard deviation\n",
    "\n",
    "Implementation Note: You will do this for all features and your code should work with datasets of all sizes (any number of features/ examples). After learning the parameters from the model, you must first normalize the new $x$ value using the mean and standard deviation that you have previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to use the following package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix # optional\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code goes here for this section.\n",
    "X_train = [];\n",
    "y_train = [];\n",
    "X_test = [];\n",
    "y_test = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 3. IMPLEMENTATION OF GRADIENT DESCENT (45 pts)\n",
    "The gradient descent formulation remain the same as one in the lecture. Keep in mind that you will need to add a column $\\textbf{x}_0$ with all 1s as part of the training data. You should write code to implement the **MyLinearRegression** class and its predefined methods. \n",
    "\n",
    "* **Gradient Descent:** Notes that you may NOT call the library linear regression which defeats the purpose of this assignment. Make sure your code supports any number of features and is well-vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You may not use the library Linear Regression, but implement your own!\n",
    "# REMEMBER to place self.attribute = 0 with value from your implementation\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __int__ (self):\n",
    "        self.theta = 0; # parameter vector;\n",
    "        self.alpha = 0; # learning rate\n",
    "        self.cost  = 0; # cost function\n",
    "  \n",
    "    def fitUsingGradientDescent(self, X_train, y_train):\n",
    "        # implementation code here\n",
    "        self.theta = 0;\n",
    "    \n",
    "    def gradientDescent(self, X_train, y_train, theta, alpha, iters):\n",
    "        # INPUT:\n",
    "        # alpha: the learning rate\n",
    "        # iters: number of iterations\n",
    "        # \n",
    "        # OUTPUT: \n",
    "        # theta: updated value for theta\n",
    "        # cost: value of the cost function\n",
    "        #\n",
    "        # implementation code here\n",
    "        self.theta = 0;\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # implementation code here \n",
    "        return y_predict\n",
    "    \n",
    "    \n",
    "    def fitUsingNormalEquation(self, X_train, y_train):\n",
    "        # implementation code here for PART 4.\n",
    "        self.theta = 0;\n",
    "    \n",
    "# Your code goes here to call the instance of class MyLinearRegression\n",
    "myGradientDescentModel = MyLinearRegression()\n",
    "#myGradientDescentModel.fitUsingGradientDescent(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Learning Rate:** You will try out different learning rates for the dataset and find a learning rate that converges quickly. If you pick a learning rate, your plot of Cost Function $J(\\theta)$ against number of iteration will quickly decay to a small value. This also indicates that your implementation is correct. If your learning rate is too large, the cost function $J(\\theta)$ can diverge and blow up. From the below plot, you must be able to report the best learning rate the you found to earn credit for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to plot out your learning rate\n",
    "# iters and cost must be supplied to plot out the cost function\n",
    "# You may plot multiple curves corresponding to different learning rates to justify the best one.\n",
    "#\n",
    "# plt.set_xlabel('Iterations')  \n",
    "# plt.set_ylabel('Cost')  \n",
    "# plt.set_title('Error vs. Training Iterations')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 4. IMPLEMENTATION OF THE NORMAL EQUATION (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my lecture, you learn that the closed form solution of linear regression using the normal equation formulation. Using the formula does not require any feature scaling, and should be straight forward to implement: \n",
    "\n",
    "$\n",
    "    \\mathbf{\\theta} = ({\\mathbf{X}^{T}\\mathbf{X}})^{-1}\\mathbf{X}^{T}\\mathbf{y}.\n",
    "$\n",
    "\n",
    "Note that you still need to add a column of 1's to the $\\mathbf{\n",
    "X}$ matrix to have an intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the normalEquation method of the MyLinearRegression Class before execute the code below:\n",
    "myNormalEquationModel = MyLinearRegression()\n",
    "myGradientDescentModel.fitUsingNormalEquation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 5. COMPARISON OF DIFFERENT IMPLEMENTATIONS (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to evaluate and compare your gradient descent as well as normal equation implementation of linear regression. In theory, they should be the same, or at least similar. For good measures, you may also use the built-in library **Scholastic Gradient Descent (SGD)** as a third model for comparison. For each model, you must compute the Root Mean Squared Error (RMSE) as performance measure. The good news is that you can call library functions to compute these as shown below instead of writing your own code:\n",
    "\n",
    "* Which one yields the best performance measure for your dataset?\n",
    "* What is your assessment of the error? Good? Okay? Terrible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use the built-in SGD Regressor model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "mySGDModel = SGDRegressor()\n",
    "#mySGDModel.fit(X_train,y_train)\n",
    "#y_predict = mySGDModel.predict(X_test)\n",
    "#mse = mean_squared_error(y_test, y_predict)\n",
    "#mySGDModel_rmse = np.sqrt(mse)\n",
    "# print(mySGDModel_rmse)\n",
    "\n",
    "# myGradientDescentModel_rmse  \n",
    "# myNormalEquationModel_rmse \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 6. PRESENTATION OF YOUR SOLUTION (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you need to write a short memo of one paragraph to be read by a non-technical audience (ie. your manager/boss). Focus on answering the following: \n",
    "\n",
    "* How can you pitch your solution to this project? \n",
    "* What did you learn so far about the problem?\n",
    "* Is there any insight moving forward to improve the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your paragraph goes here for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my [slides](https://docs.google.com/presentation/d/10D1he89peAWaFgjtZlHpUzvOOAie_vIFT95htKCKgc0/edit?usp=sharing) and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during [office hour](http://bit.ly/cs4501oh)\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 1:...\".\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
