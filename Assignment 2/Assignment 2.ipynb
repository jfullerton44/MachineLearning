{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Spam Classification with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 6316 Machine Learning - Department of Computer Science - University of Virginia\n",
    "*Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In this part of the assignment, you will use SVMs to build your own spam filter. For references, you may refer to my [lecture 6](https://drive.google.com/open?id=1CeBhepjDKBaFBq2BZq-zNQs-MC8ll7aL4qAF8TJ24FM) and [lecture 6b](https://drive.google.com/open?id=13BidUAs_c2QdZkf92axt2S748sbnbI9Hgxg-fzb-OuU) or Chapter 5 of the textbook if you need additional sample codes to help with your assignment. For deliverables, you must write code in Python and submit **this** Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. PRE-PROCESSING THE DATA (15 pts)\n",
    "\n",
    "**Data Acquiring:** Download the dataset from https://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "**Data Splitting:** Put data into the format needed for SVM, then split it into 80% training, 20% testing (each should have approximately the same proportion of position and negative examples).\n",
    "\n",
    "**Data Discovery:** Plot out all correlations among the features. You may notice some features are more correlated with your predicted value than other. This information will help you confirm that weights of your regression model later on.\n",
    "\n",
    "**Data Cleaning:** If your dataset has some missing values, make sure you are able to fill those values with the Imputer class. \n",
    "\n",
    "**Feature Scaling** You can use the standard Scikit-Learn library can write some codes to normalize the value of each features as follow:\n",
    "\n",
    "* Subtract the mean value of each feature from the dataset\n",
    "* Scale (divide) the feature values by their respective standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to use the following package\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix # optional\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code goes here for this section.\n",
    "# X_train = [];\n",
    "# y_train = [];\n",
    "# X_test = [];\n",
    "# y_test = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 2. TRAINING SVM FOR SPAM CLASSIFICATION (15 pts)\n",
    "\n",
    "Run your linear SVM classifier on the training data, and test the resulting model on the test data. In this section, you may use the default **loss function** (hinge) and **default** value of the C hyperparameter (=1.0):\n",
    "\n",
    "* Report accuracy, precision, recall?\n",
    "* Create an ROC curve for this SVM on the test data, using 200 or more evenly spaced thresholds. You may use library function calls to create the ROC curve?\n",
    "\n",
    "**Implementation Notes:** You do NOT need to add a column of 1's to the $\\mathbf{\n",
    "x}$ matrix to have an intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Training your svm here\n",
    "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "\n",
    "# Testing your svm here\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 3. TUNING C FOR LINEAR SVM (15 pts)\n",
    "In this part of the assignment, you will try using different values of the C parameter with SVMs. Your task is to try different values of C on this dataset. For example, when C is small, you should find that the SVM puts the decision boundary in the gap between the two datasets and misclassifies the data point. When C is large, you should find that the SVM now classifies every single example correctly, but has a decision boundary that does not appear to be a natural fit for the data. \n",
    "\n",
    "In a 2-D feature space of your choice, you must be able to plot out the decision boundary of SVM for different values of C to earn credit for this section. Depends on your observation on the dataset, you must provide some justifications on the reason you select a certain value of C to tradeoff margin and data violation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to plot out your hyperparameter C\n",
    "# You may plot multiple decision boundary corresponding to different C\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# plt.figure(figsize=(12,3))\n",
    "# plt.subplot(121) \n",
    "# plt.xlabel(\"Feature 1\", fontsize=14)\n",
    "# plt.ylabel(\"Feature 2\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 4. SELECTING THE FEATURES WITH SVM (25 pts)\n",
    "\n",
    "Once your learned a best linear SVM in previous sections, your task in this section is to select the best features. First, you must * Obtain the weight vector $\\mathbf{w}$. Then, for the number of features $n = 2$ to 57, you will run the following (in a loop of course):\n",
    "\n",
    "* Select the set of $n$ features that have the highest $\\mathbf{w}_n$\n",
    "* Train an SVM, $SVM_n$, on all training data, only using these $n$ features with the same hyperparameter C.\n",
    "* Test $SVM_n$ on the test set (using the same $n$ features) to obtain accuracy.\n",
    "* Plot accuracy on test data vs. $n$ number of features\n",
    "\n",
    "In one paragraph, discuss the effects of feature selection including the top 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your feature selection code goes here\n",
    "\n",
    "\n",
    "# Your paragraph goes here for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 5. KERNELIZING SVM WITH THE GAUSSIAN RBF (30 pts)\n",
    "\n",
    "In this part of the asisgnment, you will be using SVMs to do non-linear classification. In particular, you will be using SVMs with Gaussian kernels on datasets that are not linearly separable. \n",
    "\n",
    "$\n",
    "    \\mathbf{K}_{RBF}(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)}) = \\exp(-\\gamma ||\\mathbf{x}^{(i)} -\\mathbf{x}^{(j)}||^2).\n",
    "$\n",
    "\n",
    "Your task is to determine the best $C$ and $\\gamma$ hyperparameters to use, run SVM on the spam data and report the performance in metrics similar to section 2. By using the Gaussian kernel with the SVM, you will be able to learn a non-linear decision boundary that can perform reasonably well for this dataset. \n",
    "\n",
    "Finally, run the version of SVM with the best hyperparameters on the test set, and plot out the comparison in terms of accuracy, precision, and recall, and the ROC curve) to those of linear SVM in Section 3. How much your SVM classifier perform better? \n",
    "\n",
    "**Implementation Note:** When implementing cross validation to select the best C and $\\gamma$ parameter to use, you need to evaluate the error on the cross validation set. Recall that for classification, the error is defined as the fraction of the cross validation examples that were classified incorrectly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2), ...\n",
    "# for gamma, C in hyperparams:\n",
    "#    rbf_kernel_svm_clf = SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "#    rbf_kernel_svm_clf.fit(X_cv, y_cv)\n",
    "#    # Your code to train and find the best value of C and gamma here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my [lecture 6](https://drive.google.com/open?id=1CeBhepjDKBaFBq2BZq-zNQs-MC8ll7aL4qAF8TJ24FM) and [lecture 6b](https://drive.google.com/open?id=13BidUAs_c2QdZkf92axt2S748sbnbI9Hgxg-fzb-OuU) and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during [office hour](https://docs.google.com/document/d/15qB84xjaS-uRJmfKmmQuCz38bLMFaoqdbuRLoZEdOYI/edit#heading=h.72k1pvft525n)\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS6316 Assignment 2:...\".\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
